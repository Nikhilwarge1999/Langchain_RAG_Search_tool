{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d93471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a86e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from wikipedia) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b457e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=wiki_wrapper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996dc743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creaated the 1 tool\n",
    "wiki.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdfa4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7bba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for the arxiv tool\n",
    "\n",
    "arxvi_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "arxiv=ArxivQueryRun(api_erapper=arxvi_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8340c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#develop the 2nd inbuilt tool\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14e43dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing the custome tool with RAG\n",
    "#1 load the API key from the .env file\n",
    "#2 Authenticate with the huggingface API key\n",
    "\n",
    "import os \n",
    "from dotenv import  load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY=os.getenv(\"HF_API_KEY\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f14393ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nikhil Warge\\OneDrive\\Documents\\GitHub\\Implementing-ANN-and-RNN-\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#login to the huggingface hub using the API key\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17dde9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#start developting the custome tool with RAG\n",
    "#load the data from the web using webbase loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "#vectore store to store the data \n",
    "from langchain_community.vectorstores import faiss\n",
    "#load the embeding technique from the huggingface hub\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "#for converting the text data into corpu or chunk \n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2437be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading embeding model from the huggingfface hub\n",
    "HF_APIKEY=os.getenv(\"HF_API_KEY\")\n",
    "embidder=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_APIKEY,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79a02e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interview_questions'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store the web data into the vector store using faiss\n",
    "loader=WebBaseLoader(\"https://www.analyticsvidhya.com/blog/2024/11/generative-ai-interview-questions/\")\n",
    "#load the data from the web using webbase loader\n",
    "document=loader.load()\n",
    "\n",
    "#2 split the data into chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=540,\n",
    "    chunk_overlap=0\n",
    "    )\n",
    "doc=text_splitter.split_documents(document)\n",
    "\n",
    "#3 Initialise the hhuggingface inferance API embedding model\n",
    "\n",
    "embedding=HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_APIKEY,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#step 4 create the vectore store using FAISS\n",
    "\n",
    "vectorestore=faiss.FAISS.from_documents(doc,embedding)\n",
    "\n",
    "#step 5 create the retriever using the vectore store\n",
    "retriever=vectorestore.as_retriever(search_kwargs={\"k\":1})\n",
    "#step 6 create the custome tool uaing the retriever and the embeding model\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool=create_retriever_tool(retriever,\"interview_questions\",\"search any information about any question\")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66dd5171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\Nikhil Warge\\\\OneDrive\\\\Documents\\\\GitHub\\\\Implementing-ANN-and-RNN-\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " Tool(name='interview_questions', description='search any information about any question', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000021F94503100>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInferenceAPIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021FA3DB84D0>, search_kwargs={'k': 1}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000021F9482A8E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInferenceAPIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021FA3DB84D0>, search_kwargs={'k': 1}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now chain the tools together using the langchain agent\n",
    "\n",
    "tool=[wiki,arxiv,retriever_tool]\n",
    "\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3febdb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021FDDDD27D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021FDDDCC090>, model_name='llama-3.3-70b-versatile', temperature=0.1, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now rull all the tools with agents and the LLM model\n",
    "\n",
    "##Tools , LLM >>Agent Executer\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import  load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "groq_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_key,model=\"llama-3.3-70b-versatile\",temperature=0.1)\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae89d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-openai) (0.3.47)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.66.3 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-openai) (1.67.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.3.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikhil warge\\onedrive\\documents\\github\\implementing-ann-and-rnn-\\venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.66.3->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8deb91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nikhil Warge\\OneDrive\\Documents\\GitHub\\Implementing-ANN-and-RNN-\\venv\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df433ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51f2dd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000021F942704A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000021F942704A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021FDDDD27D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021FDDDCC090>, model_name='llama-3.3-70b-versatile', temperature=0.1, model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'interview_questions', 'description': 'search any information about any question', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the AGENTS\n",
    "\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tool,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4c102f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Shivaji Maharaj'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Shivaji\n",
      "Summary: Shivaji I (Shivaji Shahaji Bhonsale, Marathi pronunciation: [ʃiˈʋaːdʑiː ˈbʱos(ə)le]; c. 19 February 1630 – 3 April 1680) was an Indian ruler and a member of the Bhonsle dynasty. Shivaji carved out his own independent kingdom fr\u001b[0m\u001b[32;1m\u001b[1;3mShivaji Maharaj, also known as Chhatrapati Shivaji, was a Indian ruler and a member of the Bhonsle dynasty. He was born on February 19, 1630, and died on April 3, 1680. Shivaji is known for carving out his own independent kingdom from the Mughal Empire and establishing the Maratha Empire. He is considered one of the greatest rulers in Indian history and is revered as a hero and a symbol of Indian nationalism.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Shivaji Maharaj',\n",
       " 'output': 'Shivaji Maharaj, also known as Chhatrapati Shivaji, was a Indian ruler and a member of the Bhonsle dynasty. He was born on February 19, 1630, and died on April 3, 1680. Shivaji is known for carving out his own independent kingdom from the Mughal Empire and establishing the Maratha Empire. He is considered one of the greatest rulers in Indian history and is revered as a hero and a symbol of Indian nationalism.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define agent executor\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executer=AgentExecutor(agent=agent,tools=tool,verbose=True)\n",
    "agent_executer.invoke({\"input\":\"Shivaji Maharaj\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f77ffd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `interview_questions` with `{'query': 'Evaluating the performance of the Gen AI model'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mLearn everything there is to know about generative AI and become a GenAI expert with our GenAI Pinnacle Program.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Evaluating the performance of the Gen AI model'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Gemini (language model)\n",
      "Summary: Gemini is a family of multimodal large language models developed by Google DeepMind, and the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'Evaluating the performance of the Gen AI model'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2025-02-12\n",
      "Title: Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning\n",
      "Authors: Zijian He, Reyna Abhyankar, Vikranth Srivatsa, Yiying Zhang\n",
      "Summary: Today's gen-AI workflows that involve multiple ML model calls, tool/API\n",
      "calls, data retrieval, or generic code execution are often tuned manually in an\n",
      "ad-hoc way that is both time-consuming and error-prone. In this paper, we\n",
      "propose a systematic approach for automatically tuning gen-AI workflows. Our\n",
      "key insight is that gen-AI workflows can benefit from structure, operator, and\n",
      "prompt changes, but unique properties of gen-AI workflows require new\n",
      "optimization techniques. We propose AdaSeek, an adaptive hierarchical search\n",
      "algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning\n",
      "methods into different layers based on the user-specified total search budget\n",
      "and distributes the budget across different layers based on the complexity of\n",
      "each layer. During its hierarchical search, AdaSeek redistributes the search\n",
      "budget from less useful to more promising tuning configurations based on\n",
      "workflow-level evaluation results. We implement AdaSeek in a workflow\n",
      "autotuning framework called Cognify and evaluate Cognify using six types of\n",
      "workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify\n",
      "improves these workflows' generation quality by up to 2.8x, reduces execution\n",
      "monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.\n",
      "\n",
      "Published: 2025-02-13\n",
      "Title: Assessing Generative AI value in a public sector context: evidence from a field experiment\n",
      "Authors: Trevor Fitzpatrick, Seamus Kelly, Patrick Carey, David Walsh, Ruairi Nugent\n",
      "Summary: The emergence of Generative AI (Gen AI) has motivated an interest in\n",
      "understanding how it could be used to enhance productivity across various\n",
      "tasks. We add to research results for the performance impact of Gen AI on\n",
      "complex knowledge-based tasks in a public sector setting. In a pre-registered\n",
      "experiment, after establishing a baseline level of performance, we find mixed\n",
      "evidence for two types of composite tasks related to document understanding and\n",
      "data analysis. For the Documents task, the treatment group using Gen AI had a\n",
      "17% improvement in answer quality scores (as judged by human evaluators) and a\n",
      "34% improvement in task completion time compared to a control group. For the\n",
      "Data task, we find the Gen AI treatment group experienced a 12% reduction in\n",
      "quality scores and no significant difference in mean completion time compared\n",
      "to the control group. These results suggest that the benefits of Gen AI may be\n",
      "task and potentially respondent dependent. We also discuss field notes and\n",
      "lessons learned, as well as supplementary insights from a post-trial survey and\n",
      "feedback workshop with participants.\n",
      "\n",
      "Published: 2024-03-09\n",
      "Title: A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation\n",
      "Authors: Yao Lyu, He Zhang, Shuo Niu, Jie Cai\n",
      "Summary: Content creators increasingly utilize generative artificial intelligence\n",
      "(Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging\n",
      "sites to produce imaginative images, AI-generated videos, and articles using\n",
      "Large Language Models (LLMs). Despite its growing popularity, there remains an\n",
      "underexplored area concerning the specific domains where AI-generated content\n",
      "is being applied, and the methodologies content creators employ with Gen-AI\n",
      "tools during the creation process. This study initially explores this emerging\n",
      "area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI\n",
      "usage. Our research focuses on identifying the content domains, the variety of\n",
      "tools used, the activities performed, and the nature of the final products\n",
      "generated by Gen-AI in the context of user-generated content.\u001b[0m\u001b[32;1m\u001b[1;3mEvaluating the performance of the Gen AI model can be done by assessing its ability to generate high-quality content, such as text, images, or videos, and its ability to complete tasks efficiently. The performance of Gen AI can be evaluated using various metrics, such as answer quality scores, task completion time, and user satisfaction.\n",
      "\n",
      "In a field experiment, researchers found that Gen AI improved answer quality scores by 17% and task completion time by 34% for a document understanding task, but had mixed results for a data analysis task. Another study explored the use of Gen AI in content creation on YouTube and found that creators used Gen AI tools to produce imaginative images, AI-generated videos, and articles.\n",
      "\n",
      "To evaluate the performance of Gen AI, researchers and developers can use various methods, such as:\n",
      "\n",
      "1. Human evaluation: Human evaluators can assess the quality of the generated content and provide feedback.\n",
      "2. Automated metrics: Automated metrics, such as answer quality scores and task completion time, can be used to evaluate the performance of Gen AI.\n",
      "3. Comparison with human performance: Gen AI's performance can be compared with human performance on the same tasks to evaluate its effectiveness.\n",
      "4. User feedback: User feedback and satisfaction can be used to evaluate the performance of Gen AI in real-world applications.\n",
      "\n",
      "Overall, evaluating the performance of Gen AI requires a combination of human evaluation, automated metrics, and user feedback to assess its ability to generate high-quality content and complete tasks efficiently.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me how to evaluate the performance of the Gen AI model?',\n",
       " 'output': \"Evaluating the performance of the Gen AI model can be done by assessing its ability to generate high-quality content, such as text, images, or videos, and its ability to complete tasks efficiently. The performance of Gen AI can be evaluated using various metrics, such as answer quality scores, task completion time, and user satisfaction.\\n\\nIn a field experiment, researchers found that Gen AI improved answer quality scores by 17% and task completion time by 34% for a document understanding task, but had mixed results for a data analysis task. Another study explored the use of Gen AI in content creation on YouTube and found that creators used Gen AI tools to produce imaginative images, AI-generated videos, and articles.\\n\\nTo evaluate the performance of Gen AI, researchers and developers can use various methods, such as:\\n\\n1. Human evaluation: Human evaluators can assess the quality of the generated content and provide feedback.\\n2. Automated metrics: Automated metrics, such as answer quality scores and task completion time, can be used to evaluate the performance of Gen AI.\\n3. Comparison with human performance: Gen AI's performance can be compared with human performance on the same tasks to evaluate its effectiveness.\\n4. User feedback: User feedback and satisfaction can be used to evaluate the performance of Gen AI in real-world applications.\\n\\nOverall, evaluating the performance of Gen AI requires a combination of human evaluation, automated metrics, and user feedback to assess its ability to generate high-quality content and complete tasks efficiently.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executer.invoke({\"input\":\"Tell me how to evaluate the performance of the Gen AI model?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50aebe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c25a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
